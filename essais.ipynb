{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste = [#\"1erNovembre2.csv\",\n",
    "\"1erNovembre3.csv\",\n",
    "\"2018-11-02 16:41.csv\",\n",
    "\"2NovembreBesac.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds = []\n",
    "for l in liste:\n",
    "    pds.append(pd.read_csv(l, sep = \";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_half(x):\n",
    "    if x < .5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "pred = list((pds[0][\"prediction\"] + pds[1][\"prediction\"] + pds[2][\"prediction\"])/3)\n",
    "prediction = [cut_half(x) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pds[0][\"prediction\"] = prediction\n",
    "p =pds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_index(\"name\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_csv(\"mix.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from copy import deepcopy as dp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from tqdm import *\n",
    "import os \n",
    "from os import system\n",
    "import numpy as np \n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import RandomNormal as RN\n",
    "from PIL import Image\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from scipy import misc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 128\n",
    "#img_width, img_height = size, size\n",
    "\n",
    "data_train_path_target = \"data_airbus_defi/train/\"\n",
    "data_test_path = \"data_airbus_defi/test/\"\n",
    "input_shape = (size, size, 3)\n",
    "#imput_shape = (size,size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "full = False\n",
    "some = True\n",
    "if full:\n",
    "    training_size = float(\"inf\")\n",
    "    testing_size = float(\"inf\")\n",
    "elif some:\n",
    "    training_size = 1000\n",
    "    testing_size = float(\"inf\")\n",
    "else:\n",
    "    training_size = 10\n",
    "    testing_size = 10\n",
    "    \n",
    "\n",
    "train_path = PATH +\"/\" + data_train_path_target + \"target/\"\n",
    "train_data_target = os.listdir(train_path)\n",
    "x_train = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#im_train = []\n",
    "tdt = 0\n",
    "for sample in (train_data_target):\n",
    "    img_path = train_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_train.append(np.array(x))\n",
    "    \n",
    "    \"\"\"\n",
    "    im = Image.open(img_path)\n",
    "    im_grey = im.convert('L') # convert the image to *greyscale*\n",
    "    #im_bw = im_grey.point(lambda x: 0 if x<128 else 255, '1')\n",
    "    im_array = np.array(im_grey)\n",
    "    im_train.append(im_array)\n",
    "    \"\"\"\n",
    "    tdt += 1\n",
    "    \n",
    "    if tdt > training_size:\n",
    "        break\n",
    "\n",
    "train_path = PATH +\"/\" + data_train_path_target + \"other/\"\n",
    "train_data_other = os.listdir(train_path)\n",
    "x_train2 = []\n",
    "im_train2 = []\n",
    "tdo = 0\n",
    "for sample in (train_data_other):\n",
    "    img_path = train_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_train2.append(np.array(x))\n",
    "    \"\"\"\n",
    "    im = Image.open(img_path)\n",
    "    im_grey = im.convert('L') # convert the image to *greyscale*\n",
    "    #im_bw = im_grey.point(lambda x: 0 if x<128 else 255, '1')\n",
    "    im_array = np.array(im_grey)\n",
    "    im_train2.append(im_array)\n",
    "    \"\"\"\n",
    "    tdo += 1\n",
    "    if tdo > training_size:\n",
    "        break\n",
    "\n",
    "    \n",
    "test_path = PATH+'/data_airbus_defi/test/'\n",
    "test_data = os.listdir(test_path)\n",
    "x_test = []\n",
    "#im_test = []\n",
    "\n",
    "\n",
    "td = 0\n",
    "output = pd.DataFrame(columns=[\"name\"])\n",
    "test_data = [str(x) + \".jpg\" for x in range(len(test_data))]\n",
    "for sample in (test_data):\n",
    "    output.append({\"name\": sample},ignore_index=True)\n",
    "    #print(sample)\n",
    "    img_path = test_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_test.append(np.array(x))\n",
    "    \"\"\"\n",
    "    im = Image.open(img_path)\n",
    "    im_grey = im.convert('L') # convert the image to *greyscale*\n",
    "    #im_bw = im_grey.point(lambda x: 0 if x<128 else 255, '1')\n",
    "    im_array = np.array(im_grey)\n",
    "    im_test.append(im_array)\n",
    "    \"\"\"\n",
    "    td+=1\n",
    "    if td > testing_size:\n",
    "        break\n",
    "    \n",
    "    \n",
    "#test = np.array([np.array(x_t) for x_t in x_test])\n",
    "# finally converting list into numpy array\n",
    "x_train = np.array(x_train)\n",
    "x_train2 = np.array(x_train2)\n",
    "x_test = np.array(x_test) / 255.\n",
    "XTRAIN = np.concatenate((x_train, x_train2), axis=0) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label1 = np.array([1] * x_train.shape[0] + [0] * x_train2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "XTRAIN, train_labels = unison_shuffled_copies(XTRAIN,train_label1)\n",
    "#IMTRAIN, im_labels = unison_shuffled_copies(IMTRAIN,train_labels)\n",
    "#XTRAIN, train_labels  = zip(*random.shuffle(list(zip(XTRAIN,train_label1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system('say Data mélangée!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn(size, n_layers):\n",
    "    # INPUTS\n",
    "    # size     - size of the input images\n",
    "    # n_layers - number of layers\n",
    "    # OUTPUTS\n",
    "    # model    - compiled CNN\n",
    "\n",
    "    # Define hyperparamters\n",
    "    MIN_NEURONS = 3\n",
    "    MAX_NEURONS = 6\n",
    "    KERNEL = (3, 3)\n",
    "\n",
    "    # Determine the # of neurons in each convolutional layer\n",
    "    steps = np.floor(MAX_NEURONS / (n_layers + 1))\n",
    "    nuerons = np.arange(MIN_NEURONS, MAX_NEURONS, steps)\n",
    "    nuerons = nuerons.astype(np.int32)\n",
    "\n",
    "    # Define a model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(0, n_layers):\n",
    "        if i == 0:\n",
    "            shape = (size[0], size[1], size[2])\n",
    "            model.add(Conv2D(nuerons[i], KERNEL, input_shape=shape))\n",
    "        else:\n",
    "            model.add(Conv2D(nuerons[i], KERNEL))\n",
    "\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    # Add max pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(MAX_NEURONS))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Print a summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 126, 126, 3)       84        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 126, 126, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 124, 124, 4)       112       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 124, 124, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 122, 122, 5)       185       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 122, 122, 5)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 61, 61, 5)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18605)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 111636    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 112,024\n",
      "Trainable params: 112,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = cnn(size=input_shape, n_layers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 200\n",
    "PATIENCE = 10\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=PATIENCE, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 10\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=PATIENCE, verbose=0, mode='auto')\n",
    "LOG_DIRECTORY_ROOT = ''\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = \"TF\" + now\n",
    "tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n",
    "callbacks = [early_stopping, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a308e35f8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(XTRAIN, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20181103094232'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tensorboard --logdir TF2018......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
