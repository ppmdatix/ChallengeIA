{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets import some useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from copy import deepcopy as dp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.initializers import RandomNormal as RN\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import *\n",
    "import os \n",
    "from os import system\n",
    "from PIL import Image\n",
    "from IPython.display import SVG\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 128\n",
    "data_train_path_target = \"data_airbus_defi/train/\"\n",
    "data_test_path = \"data_airbus_defi/test/\"\n",
    "input_shape = (size, size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data. Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "\n",
    "###############\n",
    "# SIZE LOADED #\n",
    "###############\n",
    "\n",
    "full = True\n",
    "some = True\n",
    "if full:\n",
    "    training_size = float(\"inf\")\n",
    "    testing_size = float(\"inf\")\n",
    "elif some:\n",
    "    training_size = 1000\n",
    "    testing_size = float(\"inf\")\n",
    "else:\n",
    "    training_size = 10\n",
    "    testing_size = 10\n",
    "    \n",
    "\n",
    "    \n",
    "#########\n",
    "# TRAIN #\n",
    "#########\n",
    "\n",
    "train_path = PATH +\"/\" + data_train_path_target + \"target/\"\n",
    "train_data_target = os.listdir(train_path)\n",
    "x_train = []\n",
    "tdt = 0\n",
    "for sample in (train_data_target):\n",
    "    img_path = train_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    x_train.append(np.array(x))\n",
    "    tdt += 1\n",
    "    if tdt > training_size:\n",
    "        break\n",
    "\n",
    "train_path = PATH +\"/\" + data_train_path_target + \"other/\"\n",
    "train_data_other = os.listdir(train_path)\n",
    "x_train2 = []\n",
    "tdo = 0\n",
    "for sample in (train_data_other):\n",
    "    img_path = train_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_train2.append(np.array(x))\n",
    "    tdo += 1\n",
    "    if tdo > training_size:\n",
    "        break\n",
    "\n",
    "        \n",
    "########\n",
    "# TEST #\n",
    "########\n",
    "\n",
    "test_path = PATH+'/data_airbus_defi/test/'\n",
    "test_data = os.listdir(test_path)\n",
    "x_test = []\n",
    "td = 0\n",
    "output = pd.DataFrame(columns=[\"name\"])\n",
    "test_data = [str(x) + \".jpg\" for x in range(len(test_data))]\n",
    "for sample in (test_data):\n",
    "    output.append({\"name\": sample},ignore_index=True)\n",
    "    #print(sample)\n",
    "    img_path = test_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_test.append(np.array(x))\n",
    "    td+=1\n",
    "    if td > testing_size:\n",
    "        break\n",
    "    \n",
    "\n",
    "# finally converting list into numpy array\n",
    "x_train = np.array(x_train)\n",
    "x_train2 = np.array(x_train2)\n",
    "x_test = np.array(x_test) / 255.\n",
    "XTRAIN = np.concatenate((x_train, x_train2), axis=0) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system('say Data chargée!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape\n",
      "(32966, 128, 128, 3)\n",
      "(35252, 128, 128, 3)\n",
      "(68218, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data shape\")\n",
    "print(x_train.shape)\n",
    "print(x_train2.shape)\n",
    "print(XTRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lablelling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label1 = np.array([1] * x_train.shape[0] + [0] * x_train2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffling data. Not (totally) sure it is useful.\n",
    "\n",
    "Quite ong when we use every data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "XTRAIN, train_labels = unison_shuffled_copies(XTRAIN,train_label1)\n",
    "#IMTRAIN, im_labels = unison_shuffled_copies(IMTRAIN,train_labels)\n",
    "#XTRAIN, train_labels  = zip(*random.shuffle(list(zip(XTRAIN,train_label1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system('say Data mélangée!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful (?) functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotModel(model):\n",
    "    SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "def plotLearning(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    #plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    #plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def saveModel(model, name):\n",
    "    path = \"model/\" + name \n",
    "    model_json = model.to_json()\n",
    "    with open(path + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(path + \".h5\")\n",
    "    print(\"Saved model\")\n",
    "\n",
    "def loadModel( name):\n",
    "    path = \"model/\" + name \n",
    "    json_file = open(path + \".json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(path + \".h5\")\n",
    "    #loaded_model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def soumissionCSV(prediction, name):\n",
    "    global test_data\n",
    "    X = pd.DataFrame()\n",
    "    X[\"name\"] = test_data\n",
    "    X[\"prediction\"] = prediction\n",
    "    X.set_index(\"name\", inplace=True)\n",
    "    X.to_csv(name + \".csv\", sep=\";\")\n",
    "    print(\"CSV file written\")\n",
    "    \n",
    "def cut_half(x):\n",
    "    if x < .5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def prediction_from_model(model):\n",
    "    global x_test\n",
    "    pred = model.predict(x_test)\n",
    "    prediction = [cut_half(x) for x in pred]\n",
    "    print(\"some predictions\")\n",
    "    print(prediction[:9])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn(n_layers):\n",
    "    global input_shape\n",
    "    # INPUTS\n",
    "    # size     - size of the input images\n",
    "    # n_layers - number of layers\n",
    "    # OUTPUTS\n",
    "    # model    - compiled CNN\n",
    "\n",
    "    # Define hyperparamters\n",
    "    MIN_NEURONS = 12\n",
    "    MAX_NEURONS = 32\n",
    "    KERNEL = (3, 3)\n",
    "\n",
    "    # Determine the # of neurons in each convolutional layer\n",
    "    steps = np.floor(MAX_NEURONS / (n_layers + 1))\n",
    "    nuerons = np.arange(MIN_NEURONS, MAX_NEURONS, steps)\n",
    "    nuerons = nuerons.astype(np.int32)\n",
    "\n",
    "    # Define a model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(0, n_layers):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(nuerons[i], KERNEL, input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(Conv2D(nuerons[i], KERNEL))\n",
    "\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    # Add max pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(MAX_NEURONS))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Print a summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 126, 126, 12)      336       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 126, 126, 12)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 124, 124, 20)      2180      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 124, 124, 20)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 122, 122, 28)      5068      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 122, 122, 28)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 61, 61, 28)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 104188)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                3334048   \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,341,665\n",
      "Trainable params: 3,341,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelCNN = cnn(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model. Finally !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 126, 126, 24)      672       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 124, 124, 12)      2604      \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 122, 122, 6)       654       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 40, 40, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 38, 38, 24)        1320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 24)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                221248    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 226,563\n",
      "Trainable params: 226,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained = False\n",
    "\n",
    "epochs = 10\n",
    "validation_split = .1\n",
    "conv2Dsize = 24\n",
    "pool_size = (3,3)\n",
    "fulSize = 64\n",
    "\n",
    "\n",
    "modelBW = Sequential()\n",
    "modelBW.add(Conv2D(conv2Dsize, pool_size, input_shape = input_shape, activation = 'relu'))\n",
    "\n",
    "\n",
    "modelBW.add(Conv2D(12, pool_size, activation = 'relu'))\n",
    "modelBW.add(Conv2D( 6, pool_size, activation = 'relu'))\n",
    "\n",
    "\n",
    "modelBW.add(MaxPooling2D(pool_size = pool_size))\n",
    "#model.add(Dropout(0.2))\n",
    "modelBW.add(Conv2D(conv2Dsize, (3, 3), activation = 'relu'))\n",
    "modelBW.add(MaxPooling2D(pool_size = pool_size))\n",
    "#model.add(Dropout(0.2))\n",
    "modelBW.add(Flatten())\n",
    "modelBW.add(Dense(units = fulSize, activation = 'sigmoid'))\n",
    "modelBW.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "modelBW.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "modelBW.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe we can une Tensorboard to vizualize learning ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PATIENCE = 10\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=PATIENCE, verbose=0, mode='auto')\n",
    "LOG_DIRECTORY_ROOT = ''\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_dir = \"TF\" + now\n",
    "tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n",
    "callbacks = [early_stopping, tensorboard]\n",
    "model.fit(XTRAIN, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61396 samples, validate on 6822 samples\n",
      "Epoch 1/10\n",
      "61396/61396 [==============================] - 3221s 52ms/step - loss: 0.2462 - acc: 0.9050 - val_loss: 0.1765 - val_acc: 0.9317\n",
      "Epoch 2/10\n",
      "35648/61396 [================>.............] - ETA: 17:03 - loss: 0.1541 - acc: 0.9381"
     ]
    }
   ],
   "source": [
    "history = modelCNN.fit(XTRAIN, train_labels, epochs=epochs, verbose=1, validation_split=validation_split)\n",
    "plotLearning(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some predictions\n",
      "[1, 0, 1, 0, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "prediction = prediction_from_model(modelBW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = str(datetime.today())[:16]\n",
    "soumissionCSV(prediction, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system('say On est bon!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of windmills\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4744957786116323"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Proportion of windmills\")\n",
    "sum(prediction)/ len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "saveModel(modelBW, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading old models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "some predictions\n",
      "[1, 0, 1, 0, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "loadedModel = loadModel(\"1erNovembre3\")\n",
    "pred = prediction_from_model(loadedModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One model that was good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "validation_split = .1\n",
    "conv2Dsize = 12\n",
    "pool_size = (2,2)\n",
    "\n",
    "\n",
    "modelBW = Sequential()\n",
    "modelBW.add(Conv2D(conv2Dsize, (5), \n",
    "                  padding=\"same\", \n",
    "                  input_shape=input_shape))\n",
    "modelBW.add(Activation(\"relu\"))\n",
    "modelBW.add(MaxPooling2D(pool_size=pool_size, strides=(2, 2)))\n",
    "\n",
    "# second set of CONV => RELU => POOL layers\n",
    "modelBW.add(Conv2D(conv2Dsize, (5, 5), padding=\"same\"))\n",
    "modelBW.add(Activation(\"relu\"))\n",
    "modelBW.add(MaxPooling2D(pool_size=pool_size, strides=(2, 2)))\n",
    "#model.add(Dropout(0.2)) ???\n",
    "\n",
    "\n",
    "\n",
    "#first (and only) set of FC => RELU layers\n",
    "modelBW.add(Flatten())\n",
    "modelBW.add(Dense(2))\n",
    "modelBW.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# softmax classifier\n",
    "modelBW.add(Dense(1))\n",
    "modelBW.add(Activation(\"relu\"))\n",
    "modelBW.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
